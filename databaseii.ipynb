{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evan Callia\n",
    "\n",
    "5/1/16\n",
    "\n",
    "Importing and setting up file readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "# sys.path.append('C:\\Python27\\ArcGIS10.3\\Lib\\site-packages')\n",
    "# import pg\n",
    "# from pg import DB\n",
    "\n",
    "complete_file = 'Building_Permits_Current.csv'\n",
    "csv_file = csv.DictReader(open(complete_file, 'rb'), delimiter=',', quotechar='\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make lists of fields for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a list of fields for each table\n",
    "permits = ['Application/Permit Number', 'Description', 'Value', 'Applicant Name', 'Application Date', 'Issue Date', 'Final Date', 'Expiration Date', 'Contractor', 'Permit and Complaint Status URL', 'Master Use Permit', 'Statuses_id', 'Permit_Types_id', 'Categories_id', 'Work_Types_id', 'Sites_id', 'Actions_id']\n",
    "statuses = ['not unique', 'id', 'Status']\n",
    "permit_types = ['not unique', 'id', 'Work Type']\n",
    "categories = ['not unique', 'id', 'Category']\n",
    "work_types = ['not unique', 'id', 'Action Type']\n",
    "sites = ['id', 'Address', 'Longitude', 'Longitude']\n",
    "actions = ['not unique', 'id', 'Action Type']\n",
    "\n",
    "#list of all the tables contianing their fields. To be iterated over later\n",
    "table_list = [permits, statuses, permit_types, categories, work_types, sites, actions]\n",
    "\n",
    "#list of the names of each table. Used to make appropriate csv file names later\n",
    "names_of_tables = ['permits', 'statuses', 'permit_types', 'categories', 'work_types', 'sites', 'actions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating entire csv into small csvs based off fields for each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#keeps track of which table we're going over so outputted csv files have appripriate names\n",
    "name_count = 0\n",
    "for table in table_list:\n",
    "    #keeps track if values can repeat\n",
    "    if \"not unique\" in table:\n",
    "        not_unique = True\n",
    "        table.remove(\"not unique\")\n",
    "    else:\n",
    "        not_unique = False\n",
    "        \n",
    "    #keeps track of value that cannot repeat\n",
    "    not_unique_values = []\n",
    "    \n",
    "    #make new file with appropriate name\n",
    "    my_file = open(names_of_tables[name_count] + '.csv','wb')\n",
    "    csvwriter = csv.DictWriter(my_file, delimiter=',', fieldnames=table)\n",
    "    csvwriter.writeheader()\n",
    "    \n",
    "    #keeps track of the id number that should be assigned in case an id field needs to be created\n",
    "    id_count = 1\n",
    "    #go though each row looking for appropriate fields for the table and populating\n",
    "    for row in csv_file:\n",
    "        #wheather the row should be written (should't be if it's a duplicate for some tables)\n",
    "        append = True\n",
    "        #new row that will be added to and appended to the csv file\n",
    "        new_row = {}\n",
    "        if \"id\" in table:\n",
    "            new_row[\"id\"] = id_count\n",
    "        for key in row:\n",
    "            if key in table:\n",
    "                if not_unique:\n",
    "                    if row[key] not in not_unique_values:\n",
    "                        #if this code runs it means that the value for this key is an unseen value for a table that should only have unique values and it should be added to the table\n",
    "                        not_unique_values.append(row[key])\n",
    "                        new_row[key] = row[key]\n",
    "                    else:\n",
    "                        #count is subtracted so it we don't skip numbers in our id column\n",
    "                        id_count -= 1\n",
    "                        #append is false becasue we've seen this value and have already put it in the table. It does not need to be again\n",
    "                        append = False\n",
    "                else:\n",
    "                    new_row[key] = row[key]\n",
    "        if append:\n",
    "            csvwriter.writerow(new_row)\n",
    "        id_count += 1\n",
    "    \n",
    "    #re-initialize dict reader so it can be parsed again\n",
    "    complete_file = 'Building_Permits_Current.csv'\n",
    "    csv_file = csv.DictReader(open(complete_file, 'rb'), delimiter=',', quotechar='\"')\n",
    "    \n",
    "    my_file.close()\n",
    "    name_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SQL to populate fk fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# db = DB(dbname = 'goeg456_callia_group_project', host = 'geog-db2.geog.uw.edu', port = 5432, user = 'evcallia', passwd = 'evcallia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Adding Foreign Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #initialize dictReaders for each csv\n",
    "# bpc_read = csv.DictReader(open(\"Building_Permits_Current.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# act_read = csv.DictReader(open(\"actions.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# cat_read = csv.DictReader(open(\"categories.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# perm_types_read = csv.DictReader(open(\"permit_types.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# # perm_read = csv.DictReader(open(\"permits_temp.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# sites_read = csv.DictReader(open(\"sites.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# stat_read = csv.DictReader(open(\"statuses.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "# work_read = csv.DictReader(open(\"work_types.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "\n",
    "# perm_writer = csv.DictWriter(open('permits.csv','wb'), delimiter=',', fieldnames=permits_temp)\n",
    "# perm_writer.writeheader() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# permit_list = []\n",
    "# for bpc_row in bpc_read:\n",
    "#     temp_dict = {}\n",
    "#     #initialize reader for loop\n",
    "#     perm_read = csv.DictReader(open(\"permits_temp.csv\", 'rb'), delimiter=',', quotechar='\"')\n",
    "#     for perm_row in perm_read:\n",
    "#         if bpc_row['Application/Permit Number'] == perm_row['Application/Permit Number']:\n",
    "#             for key in perm_row:\n",
    "#                 temp_dict[key] = perm_row[key]\n",
    "#             break\n",
    "#     for act_row in act_read:\n",
    "#         if bpc_row['Action Type'] == act_row['Action Type']:\n",
    "#             temp_dict['Actions_id'] = act_row['id']\n",
    "#     for cat_row in cat_read:\n",
    "#         if bpc_row['Category'] == cat_row['Category']:\n",
    "#             temp_dict['Categories_id'] = cat_row['id']\n",
    "#     for perm_type_row in perm_types_read:\n",
    "#         if bpc_row['Work Type'] == perm_type_row['Work Type']:\n",
    "#             temp_dict['Work_Types_id'] = perm_type_row['id']\n",
    "#     for sites_row in sites_read:\n",
    "#         if bpc_row['Address'] == sites_row['Address']:\n",
    "#             temp_dict['Sites_id'] = sites_row['id']\n",
    "#     for stat_row in stat_read:\n",
    "#         if bpc_row['Status'] == stat_row['Status']:\n",
    "#             temp_dict['Statuses_id'] = stat_row['id']\n",
    "#     for work_row in work_read:\n",
    "#         if bpc_row['Action Type'] == work_row['Action Type']: \n",
    "#             temp_dict['Actions_id'] = work_row['id']\n",
    "    \n",
    "#     #write row with keys to file\n",
    "#     perm_writer.writerow(temp_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
